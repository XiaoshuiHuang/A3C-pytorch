"""
Implementation of asynchronous advantage of actor-critic (A3C) algorithm
Creator: Xiaoshui Huang
Date: 2020-06-11
"""
import numpy as np
import torch
import torch.nn.functional as F
import math
import torch.multiprocessing as mp
import gym

GameName = 'Pendulum-v0'
class SharedAdam(torch.optim.Adam):
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,
                 weight_decay=0):
        super(SharedAdam, self).__init__(params, lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        # State initialization
        for group in self.param_groups:
            for p in group['params']:
                state = self.state[p]
                state['step'] = 0
                state['exp_avg'] = torch.zeros_like(p.data)
                state['exp_avg_sq'] = torch.zeros_like(p.data)

                # share in memory
                state['exp_avg'].share_memory_()
                state['exp_avg_sq'].share_memory_()

# a neural network used for actors and critic
class ACNetwork(torch.nn.Module):
    def __init__(self, state_dim, action_dim):
        super(ACNetwork,self).__init__()
        self.state_dim=state_dim
        self.action_dim=action_dim
        # define action generated by actor
        self.actor_hidden = torch.nn.Linear(state_dim, 200)
        self.action = torch.nn.Linear(200, action_dim)
        self.sigma = torch.nn.Linear(200, action_dim)
        # define value generated by critic
        self.critic_hidden = torch.nn.Linear(state_dim,100)
        self.value = torch.nn.Linear(100, 1)
        # initialize the weights of the above layers
        self._set_init([self.actor_hidden, self.action, self.sigma, self.critic_hidden, self.value])

    def _set_init(self, layers):
        """
        initialize the weight of neural network layers
        :param layers: neural network layers
        :return: none
        """
        for layer in layers:
            torch.nn.init.normal_(layer.weight, mean=0., std=0.1)
            torch.nn.init.constant_(layer.bias, 0.)

    def forward(self, x):
        """
        Given state x, generate actions from actor and value from critic
        :param x: input state or observation data
        :return: actions, sigma and values
        """
        actor_hidden = F.relu6(self.actor_hidden(x))
        actions = 2 * torch.tanh(self.action(actor_hidden))
        sigma = F.softplus(self.sigma(actor_hidden)) + 0.001
        critic_hidden = F.relu6(self.critic_hidden(x))
        values = self.value(critic_hidden)

        return actions, sigma, values

    def choose_action(self, x):
        """
        Given state x, generate the action by actor and a distribution
        :param s: input state or observation data
        :return:
        """
        self.training = False
        # action generated by actor
        action_net, sigma,_=self.forward(x)
        # sample from Gaussian distribution
        action = torch.distributions.Normal(action_net.view(1,).data, sigma.view(1,).data)
        action_numpy = action.sample().numpy()

        return action_numpy

    def loss(self, s, a, v_t, entropy_beat =0.005):
        """
        define loss functions for actor and critic.
        :param s: state
        :param a: action
        :param v_t: value target
        :return: total loss
        """
        self.train()
        action, sigma, values = self.forward(s)
        # calculate critic loss
        td_error = v_t - values
        critic_loss = td_error.pow(2)

        # calculate actor loss. This part could be replaced with other algorithm, such as PPO
        action_hist = torch.distributions.Normal(action, sigma)
        log_prob = action_hist.log_prob(a)
        # entropy = action_hist.entropy()
        entropy = 0.5 + 0.5 * math.log(2 * math.pi) + torch.log(action_hist.scale)
        exp_v = log_prob * td_error.detach() + entropy_beat * entropy
        actor_loss = -exp_v

        # calculate total loss
        total_loss = (critic_loss + actor_loss).mean()
        return total_loss

class Worker(mp.Process):
    """
    a class to generate asynchronous workers
    """
    def __init__(self, gnet, opt, global_ep, global_ep_r, res_queue, i):
        """
        :param gnet: global network
        :param opt: optimization network
        :param global_ep: global_episodes
        :param global_ep_r: global_episodes reward
        :param res_queue:
        :param i:
        """
        super(Worker, self).__init__()
        self.name = 'w_%i' %i
        self. g_ep, self.g_ep_r, self.res_queue = global_ep, global_ep_r, res_queue
        self.gnet, self.opt = gnet, opt
        env = gym.make(GameName)
        state_space_dim = env.observation_space.shape[0]
        action_space_dim = env.action_space.shape[0]
        self.lnet = ACNetwork(state_space_dim, action_space_dim)
        self.env = env.unwrapped
        # parameters
        self.UPDATE_GLOBAL_ITER = 5
        self.GAMMA = 0.9
        self.MAX_EP = 3000
        self.MAX_EP_STEP = 200
    def v_wrap_(self, np_array, dtype=np.float32):
        if np_array.dtype != dtype:
            np_array = np_array.astype(dtype)
        return torch.from_numpy(np_array)

    def push_and_pull_(self, opt, lnet, gnet, done, s_, bs, ba, br, gamma):
        if done:
            v_s_ = 0.0
        else:
            v_s_ = lnet.forward(self.v_wrap_(s_[None, :]))[-1].data.numpy()[0, 0]

        buffer_v_target = []
        for r in br[: : -1]:
            v_s_ = r + gamma * v_s_
            buffer_v_target.append(v_s_)
        buffer_v_target.reverse()

        s = self.v_wrap_(np.stack(bs))
        a = self.v_wrap_(np.array(ba, dtype=np.int64)) if ba[0].dtype == np.int64 else self.v_wrap_(np.vstack(ba))
        v_t = self.v_wrap_(np.array(buffer_v_target)[:, None])
        loss = lnet.loss(s, a, v_t)

        # calculate local gradients and push local parameters to global
        opt.zero_grad()
        loss.backward()
        for lp, gp in zip(lnet.parameters(), gnet.parameters()):
            gp._grad = lp.grad
        opt.step()

        # pull global parameters
        lnet.load_state_dict(gnet.state_dict())

    def print_record_(self, global_ep, global_ep_r, ep_r, res_queue, name):
        with global_ep.get_lock():
            global_ep.value += 1
        with global_ep_r.get_lock():
            if global_ep_r.value == 0.:
                global_ep_r.value = ep_r
            else:
                global_ep_r.value = global_ep_r.value * 0.99 + ep_r * 0.01
        res_queue.put(global_ep_r.value)
        print(
            name,
            "Ep:", global_ep.value,
            "| Ep_r: %.0f" % global_ep_r.value,
        )

    def run(self):
        total_step = 1
        while self.g_ep.value < self.MAX_EP:
            s = self.env.reset()
            buffer_s, buffer_a, buffer_r = [], [], []
            ep_r = 0.0
            for t in range(self.MAX_EP_STEP):
                if self.name == 'w_0':
                    self.env.render()
                a = self.lnet.choose_action(self.v_wrap_(s[None, :]))
                s_, r, done, _ = self.env.step(a.clip(-2,2))
                if t == self.MAX_EP_STEP - 1:
                    done = True
                ep_r +=r
                buffer_a.append(a)
                buffer_s.append(s)
                buffer_r.append((r+8.1)/8.1)

                # update global and assign to local network
                if total_step % self.UPDATE_GLOBAL_ITER == 0 or done:
                    # sync
                    self.push_and_pull_(self.opt, self.lnet, self.gnet, done, s_, buffer_s, buffer_a, buffer_r, self.GAMMA)
                    buffer_s, buffer_a, buffer_r = [], [], []

                    if done:
                        self.print_record_(self.g_ep, self.g_ep_r, ep_r, self.res_queue, self.name)
                        break
                s = s_
                total_step += 1

        self.res_queue.put(None)


if __name__ == "__main__":
    env = gym.make(GameName)
    N_S = env.observation_space.shape[0]
    N_A = env.action_space.shape[0]
    gnet = ACNetwork(N_S, N_A)        # global network
    gnet.share_memory()         # share the global parameters in multiprocessing
    opt = SharedAdam(gnet.parameters(), lr=1e-4, betas=(0.95, 0.999))  # global optimizer
    global_ep, global_ep_r, res_queue = mp.Value('i', 0), mp.Value('d', 0.), mp.Queue()

    # parallel training
    workers = [Worker(gnet, opt, global_ep, global_ep_r, res_queue, i) for i in range(mp.cpu_count())]
    [w.start() for w in workers]
    res = []                    # record episode reward to plot
    while True:
        r = res_queue.get()
        if r is not None:
            res.append(r)
        else:
            break
    [w.join() for w in workers]

    import matplotlib.pyplot as plt
    plt.plot(res)
    plt.ylabel('Moving average ep reward')
    plt.xlabel('Step')
    plt.show()

